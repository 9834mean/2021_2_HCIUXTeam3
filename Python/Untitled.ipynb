{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ebdfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "# 데이터를 가져온 시간을 적어주는 부분\n",
    "date = str(datetime.now())\n",
    "date = date[:date.rfind(':')].replace(' ', '_')\n",
    "date = date.replace(':','시') + '분'\n",
    "\n",
    "headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\" }\n",
    "category_list = ['정치', '경제', '사회', '생활/문화', '세계', 'IT/과학']\n",
    "#query = input('검색 키워드를 입력하세요 : ') \n",
    "news_num = int(input('총 필요한 뉴스기사 수를 입력해주세요(숫자만 입력) : '))\n",
    "#query = query.replace(' ', '+')\n",
    "\n",
    "\n",
    "#news_url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}'\n",
    "news_url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=100'\n",
    "\n",
    "req = requests.get(news_url, headers=headers)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "\n",
    "news_dict = {}\n",
    "idx = 0\n",
    "cur_page = 1\n",
    "\n",
    "print()\n",
    "print('크롤링 중...')\n",
    "\n",
    "while idx < news_num:\n",
    "### 네이버 뉴스 웹페이지 구성이 바뀌어 태그명, class 속성 값 등을 수정함(20210126) ###\n",
    "    try : \n",
    "        table = soup.find('ul',{'class' : 'list_news'})\n",
    "        li_list = table.find_all('li', {'id': re.compile('sp_nws.*')})\n",
    "        area_list = [li.find('div', {'class' : 'news_area'}) for li in li_list]\n",
    "        a_list = [area.find('a', {'class' : 'news_tit'}) for area in area_list]\n",
    "\n",
    "        for n in a_list[:min(len(a_list), news_num-idx)]:\n",
    "            news_dict[idx] = {'title' : n.get('title'),\n",
    "                              'url' : n.get('href') }\n",
    "            idx += 1\n",
    "\n",
    "        cur_page += 1\n",
    "\n",
    "        pages = soup.find('div', {'class' : 'sc_page_inner'})\n",
    "        next_page_url = [p for p in pages.find_all('a') if p.text == str(cur_page)][0].get('href')\n",
    "\n",
    "        req = requests.get('https://search.naver.com/search.naver' + next_page_url)\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    except Exception as e: \n",
    "        continue\n",
    "\n",
    "print('크롤링 완료')\n",
    "\n",
    "print('데이터프레임 변환')\n",
    "news_df = DataFrame(news_dict).T\n",
    "\n",
    "folder_path = os.getcwd()\n",
    "xlsx_file_name = '네이버뉴스_{}_{}.xlsx'.format(query, date)\n",
    "\n",
    "news_df.to_excel(xlsx_file_name)\n",
    "\n",
    "print('엑셀 저장 완료 | 경로 : {}\\\\{}'.format(folder_path, xlsx_file_name))\n",
    "os.startfile(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e17d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 필요한 뉴스기사 수를 입력해주세요(숫자만 입력) : 100\n",
      "하나를 입력하세요. : 정치\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dc96ef721f9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mlist_href\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_href\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-dc96ef721f9a>\u001b[0m in \u001b[0;36mget_href\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"list_body newsflash_body\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"photo\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"href\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def get_request(section) : \n",
    "    headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\" }\n",
    "    \n",
    "    sections = {\n",
    "        '정치' : 100,\n",
    "        '경제' : 101,\n",
    "        '사회' : 102,\n",
    "        '생활/문화' : 103,\n",
    "        '세계' : 104,\n",
    "        'IT/과학' : 105\n",
    "    }\n",
    "    \n",
    "    url = 'https://news.naver.com/main/main/list.nhn'\n",
    "    \n",
    "    req = requests.get(url, headers=headers, params={\"sid1\":sections[section]})\n",
    "    \n",
    "    return req\n",
    "\n",
    "def get_href(soup) : \n",
    "    result = []\n",
    "    \n",
    "    div = soup.find(\"div\", class_=\"list_body newsflash_body\")\n",
    "    \n",
    "    for dt in div.find_all(\"dt\", class_=\"photo\") : \n",
    "        result.append(dt.find(\"a\")[\"href\"])\n",
    "        \n",
    "    return result \n",
    "\n",
    "\n",
    "\n",
    "# 데이터를 가져온 시간을 적어주는 부분\n",
    "date = str(datetime.now())\n",
    "date = date[:date.rfind(':')].replace(' ', '_')\n",
    "date = date.replace(':','시') + '분'\n",
    "\n",
    "news_num = int(input('총 필요한 뉴스기사 수를 입력해주세요(숫자만 입력) : '))\n",
    "\n",
    "section = input(\"하나를 입력하세요. : \")\n",
    "\n",
    "req = get_request(section)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "list_href = get_href(soup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
